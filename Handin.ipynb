{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "## 1.1 Introduction\n",
    "Reddit is a large forum visited by millions of people every day, where all content is user curated. To structure this vast website, reddit is separated into sub forums called subreddits and there exists a subreddit for almost everything imagineable. These subreddits are created by the users and are therefore not a static set of categories like you might see on a news website. A user can subscribe to a set of subreddits to customize the posts they will receive. The user can create posts and comment on posts by other users. As the subreddits are not hierarchically structure, there is no central overview for users to explore new subreddits. This project will analyze the activity patterns of reddit users to identify such meta communities between subreddits. If you wish to explore one of the subreddits we mention, then you can access any subreddit by going to *www.reddit.com/r/'NameOfSubreddit'*  For a video explanation of how reddit works, watch this excellent video by CGP Grey: https://www.youtube.com/watch?v=tlI022aUWQQ.\n",
    "\n",
    "This notebook will contain the most important code snippets for the project, but not complete blocks of runnable code. \n",
    "\n",
    "## 1.2 The dataset\n",
    "Due to the immense scale of Reddit, some compromises have had to be made in data selection and data processing. The data of this project is based on the month of september 2017 and consists of posts and comments for this time period only. It was chosen to limit the data to a single month to make data processing somewhat manageable, as the entire dataset is multiple terabytes. September was chosen because reddit inflates a lot during the summer month, due to the influx of users, with a lot of time on their hands. As reddit is a website many of its users visit many times a week, we assessed that one month of data would be sufficient for the analysis. \n",
    "\n",
    "A comment data point contains the text body, author, score and the name of the subreddit it was submitted to. \n",
    "A post data point contains the title, author, score, whether the post was marked 'not safe for work (NSFW)' and the subreddit it was submitted to. \n",
    "        \n",
    "The dataset of comments is used to analyse the activity of the subreddits. The activity of a subreddit can be quantified by the different users that submit comments to it. If two subreddits have an intersection of common users, a connection is found.\n",
    "The dataset of posts is primarily used in the text analysis. The titles of each post within a community is used to compare content. The original intent was to use the text body of the comment for this, but this approach was later abandoned to limit the amount of data to process. \n",
    "\n",
    "\n",
    "## 1.3 Goal\n",
    "The goal for this project is to find affiliations across the different subreddits through their common active users. The hypothesis is that otherwise unrelated subreddits are affiliated unbeknownst to their users, making way for interesting results about the reddit community. This project will mostly revolve around the communities and how they were found. Future analyses could go into further detail about the specific meta communities to determine traits in their collective reaction to humor, controversy or external sources used in posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic stats\n",
    "## 2.1 Initial data processing and cleaning\n",
    "The Reddit network created in this project is built from the dataset of comments which is approximately 17 GB. Note that more than half of this data is the comment text body. During the initial data processing, each subreddit name is replaced with an unique integer and a dictionary is created for later reference. This is done to increase the speed of comparisons and decrease memory usage. A set of comment authors (users) are associated with each subreddit, meaning if you submitted a comment to the given subreddit, you will appear in the set. All combinations of subreddits are iterated over and if a combination has intersecting users, an undirected edge will be saved between these. The size of this intersection dictates the weight of the edge, which will be used in later processing. This process is very intensive as the amount of combinations are equal to number of subreddits squared. This was not manageable with a syncronous python script, so a multi threaded java program was written to compute this. The program can be found here: https://github.com/ageil/redditgraph/tree/master/Java\n",
    "\n",
    "\n",
    "**Basic statistics for the full network and datasets:**\n",
    "\n",
    "Comment dataset(September 2017): \n",
    " * Size: 16.7 GB\n",
    " * Rows(amount of comments): 77.510.000\n",
    " * Total authors(users): 15.108.000\n",
    " * Total subreddits: 70.888\n",
    " \n",
    "Post dataset(September 2017):\n",
    " * Size: 1 GB\n",
    " * Rows(amount of posts): 9800000\n",
    " \n",
    "Full network:\n",
    " * Nodes: 70.888 (subreddits)\n",
    " * Edges: 225.777.872\n",
    " * Avg. degree: 2999 \n",
    " * Max degree: 47547 (for subreddit 'AskReddit') \n",
    " * Avg. edge weight: 2.6 (average amount of users in common)\n",
    " \n",
    " \n",
    "Top 10 subreddits by degree:\n",
    "1. AskReddit: 47547\n",
    "2. pics: 38281\n",
    "3. funny: 37812\n",
    "4. todayilearned: 36951\n",
    "5. videos: 35692\n",
    "6. mildlyinteresting: 35615\n",
    "7. worldnews: 35165\n",
    "8. gifs: 34105\n",
    "9. gaming: 32683\n",
    "10. Showerthoughts: 32453\n",
    " \n",
    "The graph below displays the weight distribution with log scaled axes.\n",
    "<h1><center>Weight distribution of full network</center></h1>\n",
    "![title](https://i.imgur.com/EbwloGi.png)\n",
    " \n",
    "\n",
    "## 2.2 Statistic discussion\n",
    "When you create a Reddit account you will automatically be subscribed to a default set of the most popular subreddits. It is therefore assumed that these subreddits will be among those with the highest degree. This is confirmed by the fact that all subreddits in the top 10 above are default subreddits. \n",
    "\n",
    "It is clear by the distribution of edge weights (users in common) that there are a lot of nodes (subreddits) with few users in common. Some of these edges will have to be removed in order to make further processing meaningful, the upcomming section will explain how the decision of where to cut the network was made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tools, theory and analysis.\n",
    "## 3.1 Community detection\n",
    "\n",
    "In order to perform community detection on our graph, we first had to trim our graph to a more computationally manageable size. Based on the weight distribution plotted above, we performed a few tentative cuts, leaving out edges with weights lower than $\\exp(2), \\exp(3), ..., \\exp(10)$. From these cuts, we were actually able to load the graph into NetworkX and use the Louvain community detection algorithm to assess the quality/balance of the resulting communities. These were generally fairly imbalanced, however, so we ended up deciding to retain as much information in our graph as possible, while still maintaining a reasonable measure of modularity. In this way, we ended up modelling our network using a threshold of $\\exp(3)$, corresponding to at least 20 active commenters in common between the two connected subreddits. The programming details of this process can be found [here](http://nbviewer.jupyter.org/github/ageil/redditgraph/blob/master/community_detection.ipynb).\n",
    "\n",
    "Despite the rather rough and imbalanced clustering performed in the community detection, we decided to stick with this model due to the remarkable computational efficiency of the Louvain algorithm. Perhaps future projects could examine the link communities of reddit, which would additionally allow users to appear in more than one meta community at the same time. This modelling trait would sociologically be more in spirit with the nature of Reddit, where users can freely sign up for any and all interests they may have without concern for the reactions of others.\n",
    "\n",
    "## 3.2 Betweenness Analysis\n",
    "\n",
    "\n",
    "## 3.3 Working with text\n",
    "Because of the scale of this project, it was not possible to go deeply into the text analysis as this caused excessive program running times. The text used will consist of all post titles from the network cut. The main focus of this analysis is the text similarity in a community.  The following hypothesis is proposed:\n",
    "> H1: Text similarity will be higher inside a community when compared to the entire network.\n",
    "\n",
    "This hypthosis will be investigated using term frequency-inverse document frequency (TF-IDF). This method reflects how important a word is for a given document. All titles are first cleaned for stopwords, punctuation, digits, duplicates and finally all words are converted to lower case. Punctuation is removed using a regular expression and text is parsed to the method with UTF-8 encoding. The method below is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    stopws = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = nltk.RegexpTokenizer(r'\\w+').tokenize(text) # strip punctuation\n",
    "    tokens = [token for token in tokens if token.lower() not in stopws] # strip stopwords\n",
    "    tokens = [token for token in tokens if not token.isdigit()] # strip digits\n",
    "    tokens = list(set(tokens)) # remove duplicates\n",
    "    clean_text = \" \".join(tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TF-IDF the documents and corpus will have to be defined. Each subreddit will represent a single document that is the concatenation of all post titles associated with that subreddit. The corpus will be a collection of all documents associated with the subreddits in the network cut. A TF-IDF vectorizer is used to generate a matrix of the frequency for the different words in each text. The cosine similarity between the different vectors will be found so it is possible to compare two subreddits. The code below uses the *sklearn* module to perform these computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(textDictFlat.values())\n",
    "cossim = cosine_similarity(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cossim` matrix is of size $12083 \\times 12083$, hence representing the amount of subreddits on each axis. To compare similarity within communities, the pairwise similary of subreddits in the community is found. The mean of these pairs represents the text similarity in the community. The method below is used to compute this mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def meanCossimForCommunity(i):\n",
    "    subReddits = []\n",
    "    indicies = []\n",
    "    \n",
    "    # Find subreddits in group\n",
    "    for key, val in groupDict.items():\n",
    "        if val == i:\n",
    "            subReddits.append(key)\n",
    "    print(str(len(subReddits)) + ' subreddits were found for group ' + str(i))\n",
    "    \n",
    "    # Find indicies of these subreddits corrosponding to cossim matrix\n",
    "    for sub in subReddits:\n",
    "        try:\n",
    "            indicies.append(textDictOrdered.keys().index(sub))\n",
    "        except: \n",
    "            continue\n",
    "    print(str(len(indicies)) + ' indicies were found for group ' + str(i)) \n",
    "    \n",
    "    # Iterate all pairs of subreddits within a group\n",
    "    means = []\n",
    "    for i in indicies:\n",
    "        tempMean = []\n",
    "        for j in indicies:\n",
    "            if not i == j: # Avoid diagonal of matrix where a sub is compared to itself\n",
    "                tempMean.append(cossim[i][j])\n",
    "        means.append(np.mean(tempMean))\n",
    "        \n",
    "    return np.mean(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this method for the four biggest communities yields the following results:\n",
    "![title](https://i.imgur.com/cO5cCxZ.png)\n",
    "Most noticeable is that community 2 has a lower similarity than the mean of the entire network. Community 2 contains approx. 63% of the total subreddits (nodes), hence catching a substantially broader varity of text. Community 2 also contains most of the default subreddits which makes the text even less reliable, as many different users post there. These results could be improved if the Louvain community detection algorithm was better at splitting this very large community.\n",
    "\n",
    "The 3 other communities are all above the total mean, hence containing more internally similar submission titles. Therefore we fail to reject the hypothesis H1, hence the results don't prove H1 false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are satisfied with the way we managed to build the network and cut it according to the edge weights. The communities we found seem meaningful for the subreddits they contain, though we would have liked some smaller communities with greater similarity in the text. As everyone in this group knows reddit, we expected some specific subreddits to be more important in the network. In the centrality analysis we found that this was indeed the case for both degree and betweenness centrality. \n",
    "\n",
    "Generally we would have liked to go deeper with the text analysis and look at topic modelling for the communities. It would also have been interesting to process comment text instead of titles, but we were too limited in computing power to fulfill this goal. To compromise for the lack of text analysis on the website, we used wordclouds to illustrate the communities' betweenness centrality and the different websites reddit posts refer to, thus still shedding some insight on the actual content of our meta communities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
